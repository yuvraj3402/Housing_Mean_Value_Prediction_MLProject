{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to an error with the 'pyzmq' module. Consider re-installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresPyzmq'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from housing.exception import HousingException\n",
    "from housing.logger import logging\n",
    "from housing.entity.config_entity import DataTransformationConfig \n",
    "from housing.entity.artifact_entity import DataIngestionArtifact,\\\n",
    "DataValidationArtifact,DataTransformationArtifact\n",
    "import sys,os\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from housing.constatnt import *\n",
    "from housing.util.util import read_yaml_file,save_object,save_numpy_array_data,load_data\n",
    "\n",
    "\n",
    "#   longitude: float\n",
    "#   latitude: float\n",
    "#   housing_median_age: float\n",
    "#   total_rooms: float\n",
    "#   total_bedrooms: float\n",
    "#   population: float\n",
    "#   households: float\n",
    "#   median_income: float\n",
    "#   median_house_value: float\n",
    "#   ocean_proximity: category\n",
    "#   income_cat: float\n",
    "\n",
    "\n",
    "class FeatureGenerator(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, add_bedrooms_per_room=True,\n",
    "                 total_rooms_ix=3,\n",
    "                 population_ix=5,\n",
    "                 households_ix=6,\n",
    "                 total_bedrooms_ix=4, columns=None):\n",
    "        \"\"\"\n",
    "        FeatureGenerator Initialization\n",
    "        add_bedrooms_per_room: bool\n",
    "        total_rooms_ix: int index number of total rooms columns\n",
    "        population_ix: int index number of total population columns\n",
    "        households_ix: int index number of  households columns\n",
    "        total_bedrooms_ix: int index number of bedrooms columns\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.columns = columns\n",
    "            if self.columns is not None:\n",
    "                total_rooms_ix = self.columns.index(COLUMN_TOTAL_ROOMS)\n",
    "                population_ix = self.columns.index(COLUMN_POPULATION)\n",
    "                households_ix = self.columns.index(COLUMN_HOUSEHOLDS)\n",
    "                total_bedrooms_ix = self.columns.index(COLUMN_TOTAL_BEDROOM)\n",
    "\n",
    "            self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "            self.total_rooms_ix = total_rooms_ix\n",
    "            self.population_ix = population_ix\n",
    "            self.households_ix = households_ix\n",
    "            self.total_bedrooms_ix = total_bedrooms_ix\n",
    "        except Exception as e:\n",
    "            raise HousingException(e, sys) from e\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        try:\n",
    "            room_per_household = X[:, self.total_rooms_ix] / \\\n",
    "                                 X[:, self.households_ix]\n",
    "            population_per_household = X[:, self.population_ix] / \\\n",
    "                                       X[:, self.households_ix]\n",
    "            if self.add_bedrooms_per_room:\n",
    "                bedrooms_per_room = X[:, self.total_bedrooms_ix] / \\\n",
    "                                    X[:, self.total_rooms_ix]\n",
    "                generated_feature = np.c_[\n",
    "                    X, room_per_household, population_per_household, bedrooms_per_room]\n",
    "            else:\n",
    "                generated_feature = np.c_[\n",
    "                    X, room_per_household, population_per_household]\n",
    "\n",
    "            return generated_feature\n",
    "        except Exception as e:\n",
    "            raise HousingException(e, sys) from e\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "\n",
    "    def __init__(self, data_transformation_config: DataTransformationConfig,\n",
    "                 data_ingestion_artifact: DataIngestionArtifact,\n",
    "                 data_validation_artifact: DataValidationArtifact\n",
    "                 ):\n",
    "        try:\n",
    "            logging.info(f\"{'>>' * 30}Data Transformation log started.{'<<' * 30} \")\n",
    "            self.data_transformation_config= data_transformation_config\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_validation_artifact = data_validation_artifact\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "    \n",
    "\n",
    "    def get_data_transformer_object(self)->ColumnTransformer:\n",
    "        try:\n",
    "            schema_file_path = self.data_validation_artifact.schema_file_path\n",
    "\n",
    "            dataset_schema = read_yaml_file(file_path=schema_file_path)\n",
    "\n",
    "            numerical_columns = dataset_schema[NUMERICAL_COLUMN_KEY]\n",
    "            categorical_columns = dataset_schema[CATEGORICAL_COLUMN_KEY]\n",
    "\n",
    "\n",
    "            num_pipeline = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "                ('feature_generator', FeatureGenerator(\n",
    "                    add_bedrooms_per_room=self.data_transformation_config.add_bedroom_per_room,\n",
    "                    columns=numerical_columns\n",
    "                )),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]\n",
    "            )\n",
    "\n",
    "            cat_pipeline = Pipeline(steps=[\n",
    "                 ('impute', SimpleImputer(strategy=\"most_frequent\")),\n",
    "                 ('one_hot_encoder', OneHotEncoder()),\n",
    "                 ('scaler', StandardScaler(with_mean=False))\n",
    "            ]\n",
    "            )\n",
    "\n",
    "            logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "            logging.info(f\"Numerical columns: {numerical_columns}\")\n",
    "\n",
    "\n",
    "            preprocessing = ColumnTransformer([\n",
    "                ('num_pipeline', num_pipeline, numerical_columns),\n",
    "                ('cat_pipeline', cat_pipeline, categorical_columns),\n",
    "            ])\n",
    "            return preprocessing\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e   \n",
    "\n",
    "\n",
    "    def initiate_data_transformation(self)->DataTransformationArtifact:\n",
    "        try:\n",
    "            logging.info(f\"Obtaining preprocessing object.\")\n",
    "            preprocessing_obj = self.get_data_transformer_object()\n",
    "\n",
    "\n",
    "            logging.info(f\"Obtaining training and test file path.\")\n",
    "            train_file_path = self.data_ingestion_artifact.train_file_path\n",
    "            test_file_path = self.data_ingestion_artifact.test_file_path\n",
    "            \n",
    "\n",
    "            schema_file_path = self.data_validation_artifact.schema_file_path\n",
    "            \n",
    "            logging.info(f\"Loading training and test data as pandas dataframe.\")\n",
    "            train_df = load_data(file_path=train_file_path, schema_file_path=schema_file_path)\n",
    "            \n",
    "            test_df = load_data(file_path=test_file_path, schema_file_path=schema_file_path)\n",
    "\n",
    "            schema = read_yaml_file(file_path=schema_file_path)\n",
    "\n",
    "            target_column_name = schema[TARGET_COLUMN_KEY]\n",
    "\n",
    "\n",
    "            logging.info(f\"Splitting input and target feature from training and testing dataframe.\")\n",
    "            input_feature_train_df = train_df.drop(columns=[target_column_name],axis=1)\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "\n",
    "            input_feature_test_df = test_df.drop(columns=[target_column_name],axis=1)\n",
    "            target_feature_test_df = test_df[target_column_name]\n",
    "            \n",
    "\n",
    "            logging.info(f\"Applying preprocessing object on training dataframe and testing dataframe\")\n",
    "            input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)\n",
    "\n",
    "\n",
    "            train_arr = np.c_[ input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "            \n",
    "            transformed_train_dir = self.data_transformation_config.transformed_train_dir\n",
    "            transformed_test_dir = self.data_transformation_config.transformed_test_dir\n",
    "\n",
    "            train_file_name = os.path.basename(train_file_path).replace(\".csv\",\".npz\")\n",
    "            test_file_name = os.path.basename(test_file_path).replace(\".csv\",\".npz\")\n",
    "\n",
    "            transformed_train_file_path = os.path.join(transformed_train_dir, train_file_name)\n",
    "            transformed_test_file_path = os.path.join(transformed_test_dir, test_file_name)\n",
    "\n",
    "            logging.info(f\"Saving transformed training and testing array.\")\n",
    "            \n",
    "            save_numpy_array_data(file_path=transformed_train_file_path,array=train_arr)\n",
    "            save_numpy_array_data(file_path=transformed_test_file_path,array=test_arr)\n",
    "\n",
    "            preprocessing_obj_file_path = self.data_transformation_config.preprocessed_object_file_path\n",
    "\n",
    "            logging.info(f\"Saving preprocessing object.\")\n",
    "            save_object(file_path=preprocessing_obj_file_path,obj=preprocessing_obj)\n",
    "\n",
    "            data_transformation_artifact = DataTransformationArtifact(is_transformed=True,\n",
    "            message=\"Data transformation successfull.\",\n",
    "            transformed_train_file_path=transformed_train_file_path,\n",
    "            transformed_test_file_path=transformed_test_file_path,\n",
    "            preprocessed_object_file_path=preprocessing_obj_file_path\n",
    "\n",
    "            )\n",
    "            logging.info(f\"Data transformationa artifact: {data_transformation_artifact}\")\n",
    "            return data_transformation_artifact\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "    def __del__(self):\n",
    "        logging.info(f\"{'>>'*30}Data Transformation log completed.{'<<'*30} \\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
